# src/vlm_engine.py
import ollama
from .prompt_manager import PromptManager

class VLMEngine:
    def __init__(self, model_name="llava"):
        self.model = model_name
        self.prompt_manager = PromptManager()

    def analyze_scene(self, image_path):
        """
        G√∂r√ºnt√ºy√º alƒ±r ve VLM modeline g√∂nderir.
        """
        print(f"üîÑ [Sistem] G√∂r√ºnt√º i≈üleniyor: {image_path}...")
        print(f"üß† [Sistem] '{self.model}' modeli ile mantƒ±ksal analiz (CoT) ba≈ülatƒ±lƒ±yor...")

        try:
            # Ollama Python k√ºt√ºphanesi ile g√∂r√ºnt√ºy√º ve promptu g√∂nderiyoruz
            response = ollama.chat(
                model=self.model,
                messages=[
                    {
                        'role': 'system',
                        'content': self.prompt_manager.get_system_prompt(),
                    },
                    {
                        'role': 'user',
                        'content': self.prompt_manager.get_cot_prompt(),
                        'images': [image_path]
                    }
                ]
            )
            return response['message']['content']

        except Exception as e:
            return f"Hata olu≈ütu: {str(e)}"